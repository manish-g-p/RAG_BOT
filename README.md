üß† LangChain + Ollama + OpenAI | RAG & Prompt Engineering Walkthrough
This project is a hands-on walkthrough of LangChain fundamentals, local LLM integration using Ollama, and Retrieval-Augmented Generation (RAG) with OpenAI APIs. It includes prompt engineering, chaining logic, embedding generation, and document retrieval‚Äîall modular and beginner‚Äëfriendly.

Contents:
‚Ä¢ 01-langchain_Basics.ipynb ‚Äì LangChain components (LLMs, prompts, memory, chains, tools)
‚Ä¢ 02-langchain_Prompts.ipynb ‚Äì Prompt engineering with PromptTemplate, dynamic variables
‚Ä¢ 03-langchain_chains.ipynb ‚Äì LLMChain, SimpleSequentialChain, custom chain logic
‚Ä¢ Embeddings_generation_withOllama.ipynb ‚Äì Local embeddings via Ollama + FAISS vector storage
‚Ä¢ RAG_withOpenai.ipynb ‚Äì RAG pipeline: OpenAI, vector similarity, LangChain
‚Ä¢ requirements.txt ‚Äì All dependencies

Setup Instructions:

1) git clone https://github.com/your-username/langchain-rag-ollama.git && cd langchain-rag-ollama

2) python -m venv venv && source venv/bin/activate (Windows: venv\Scripts\activate)

3) pip install -r requirements.txt
   ‚ö†Ô∏è Ensure Ollama is installed and running if using local models

Features Demonstrated:
‚úÖ Core LangChain components
‚úÖ Dynamic prompts & chaining
‚úÖ Local embedding generation with Ollama
‚úÖ FAISS vector indexing
‚úÖ RAG using OpenAI APIs
‚úÖ Document splitting + retrieval pipeline

Tools & Libraries: LangChain 0.3.25, Ollama, FAISS, OpenAI API, sentence‚Äëtransformers, Python 3.11+, Jupyter
